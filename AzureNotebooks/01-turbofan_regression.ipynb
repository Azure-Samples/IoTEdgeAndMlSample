{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Introduction\n\nIn this notebook, we demonstrate the steps needed create a model for predicting Remaining Useful Life for turbofan engines based on data collected by devices and routed into storage via the IoT Hub. The notebook assumes that you have complete the device data generation steps from the [IoT  Edge for Machine Learning](aka.ms/IoTEdgeMLPaper). The data generated from the devices needs to be in an Azure Storage account blob container in the same Azure Subscription as you will use to create the Azure Machine Learning service workspace using this notebook.  \n\nThe steps we will complete in this notebooks are:\n   1. Create a Machine Learning service workspace for managing the experiments, compute, and models for this sample\n   1. Load training data from Azure Storage\n   1. Prepare the data for training the model\n   1. Explore the data \n   1. Remotely train the model\n   1. Test the model using test data\n\nThe intent is not to provide an extensive coverage of machine learning in Azure as that is covered in much depth elsewhere [here for example](https://github.com/Azure/MachineLearningNotebooks), but to demonstrate how machine learning can be used with IoT Edge."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Setup notebook\n\nPlease ensure that you are running the notebook under the Python 3.6 kernal. Intall fastavro and setup interactive shell to display output nicely.\n\n>You may see a warning about Matplotlib building the font cache. You may ignore the warning as it is benign."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install fastavro\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Move data files to data directory"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os, glob, shutil\n\nif not os.path.exists('./data'):\n    os.mkdir('./data')\n\nfor f in glob.glob('./*.txt') + glob.glob('./*.csv'):\n    shutil.move(f, './data/')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Set global properties\n\nThese properties will be used throughout the notebook.\n   * `AZURE_SUBSCRIPTION_ID` - the Azure subscription containing the storage account where device data has been uploaded. We will create the Machine Learning service workspace (ml workspace) in this subscription.\n   * `ML_WORKSPACE_NAME`  name to give the ml workspace\n   * `AZURE_IOT_HUB_NAME` - name of the Azure IoT Hub used in creating the device data using the DeviceHarness.  See [IoT  Edge for Machine Learning](aka.ms/IoTEdgeMLPaper) for details.\n   * `RESOURCE_GROUP_NAME` - name of the resource group where the IoT Hub exists\n   * `LOCATION` - the Azure location of the IoT Hub\n   * `STORAGE_ACCOUNT_NAME` - name of the Azure Storage account where device data was routed via IoT Hub.\n   * `STORAGE_ACCOUNT_KEY` - access key for the Azure Storage account\n   * `STORAGE_ACCOUNT_CONTAINER` - name of Azure Storage blob container where device data was routed via IoT Hub."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "AZURE_SUBSCRIPTION_ID = ''\nML_WORKSPACE_NAME = 'turbofanDemo'\nAZURE_IOT_HUB_NAME = ''\nRESOURCE_GROUP_NAME = ''\nLOCATION = ''\nSTORAGE_ACCOUNT_NAME = ''\nSTORAGE_ACCOUNT_KEY = ''\nSTORAGE_ACCOUNT_CONTAINER = 'devicedata'\n\nif (AZURE_SUBSCRIPTION_ID == ''\n        or ML_WORKSPACE_NAME == ''\n        or AZURE_IOT_HUB_NAME == ''\n        or RESOURCE_GROUP_NAME == ''\n        or LOCATION == ''\n        or STORAGE_ACCOUNT_NAME == ''\n        or STORAGE_ACCOUNT_KEY == ''\n        or STORAGE_ACCOUNT_CONTAINER == ''):\n    raise ValueError('All values must be filled in') ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create a workspace\n\n## What is an Azure ML Workspace and Why Do I Need One?\n\nAn Azure ML workspace is an Azure resource that organizes and coordinates the actions of many other Azure resources to assist in executing and sharing machine learning workflows. In particular, an Azure ML workspace coordinates storage, databases, and compute resources providing added functionality for machine learning experimentation, operationalization, and the monitoring of operationalized models.\n\nIn addition to creating the workspace, the cell below writes a file, config.json, to a ./aml_config/config.json, which allows the Workspace object to be reloaded later.\n\n\n\n\n><font color=gray>_Note: currently Workspaces are supported in the following regions: eastus2, eastus,westcentralus, southeastasia, westeurope, australiaeast, westus2, southcentralus_</font>\n\nYou may need to authenticate with Azure when running this cell.  If so you will see a message like: \n\n```To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code XXXXXXXXX to authenticate.```\n\nIf you are logged in with an AAD account you will instead be prompted to allow access to Azure.\n\nOnce you authenticate, the cell will finish creating the Workspace.\n\n>To facilitate rerunning the notebook with the same Workspace, the cell first checks for the presence of a config. If it finds the config, it loads the Worspace from the config instead of creating it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\nworkspace_name = ML_WORKSPACE_NAME\nsubscription_id = AZURE_SUBSCRIPTION_ID\nresource_group = RESOURCE_GROUP_NAME\nlocation = LOCATION\n\n#check to see if the workspace has already been created and persisted\nif (os.path.exists('./aml_config/config.json')):\n    ws = Workspace.from_config()\nelse:\n    ws = Workspace.create(name=workspace_name,\n                          subscription_id=subscription_id,\n                          resource_group=resource_group,\n                          create_resource_group=True,\n                          location=location\n                         )\n\n    ws.write_config(path='./aml_config')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Workspace details\n\nPrint the Workspace details."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\nimport pandas as pd\nfrom azureml.core import Workspace\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data=output, index=['']).T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Download data from storage\n\nThe first step toward creating a model for RUL is to explore the data and understand its shape. We will download the data for this purpose, realizing that in the case of larger data sets only a sample of the data would be used at this step."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Register storage account\n\nThe Datastore is a convenient construct associated the Workspace to upload/download data, and interact with it from remote compute targets. Register the Azure Storage account and container where device data was routed by IoT Hub using the information about the storage container provided at the beginning of the notebook.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Datastore\n\nds = Datastore.register_azure_blob_container(workspace=ws,\n                                             datastore_name='turbofan',\n                                             container_name=STORAGE_ACCOUNT_CONTAINER,\n                                             account_name=STORAGE_ACCOUNT_NAME,\n                                             account_key=STORAGE_ACCOUNT_KEY,\n                                             create_if_not_exists=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use Datastore to download data\n\nUse the Datastore to download the files to the local machine. The prefix is the top level path to download, which should be the name of the IoT Hub. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds.download(target_path=\"./data/download\", prefix=AZURE_IOT_HUB_NAME)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Load train data\n\nThe data we just downloaded represent a series of messages sent by each device stored in [Apache Avro](https://avro.apache.org/docs/current/)(avro) format. We will use the fastavro package to deserialize the records from the avro files.\nHere is an example deserialized record from the avro file. \n\n```json \n{\n    \"EnqueuedTimeUtc\": \"2018-12-01T01: 16: 22.0950000Z\",\n    \"Properties\": {},\n    \"SystemProperties\": {\n        \"connectionDeviceId\": \"Client_3\",\n        \"connectionAuthMethod\": {\n            \"scope\": \"device\",\n            \"type\": \"sas\",\n            \"issuer\": \"iothub\",\n            \"acceptingIpFilterRule\": null\n        },\n        \"connectionDeviceGenerationId\": \"636791290544434625\",\n        \"contentType\": \"application/json\",\n        \"contentEncoding\": \"utf-8\",\n        \"enqueuedTime\": \"2018-12-01T01: 16: 22.0950000Z\"\n    },\n    \"Body\": b'{\n        \"CycleTime\": 1,\n        \"OperationalSetting1\": -0.0001,\n        \"OperationalSetting2\": 0.0001,\n        \"OperationalSetting3\": 100.0,\n        \"Sensor1\": 518.67,\n        \"Sensor2\": 642.03,\n       //Sensor 3-19 ommitted for brevity\n        \"Sensor20\": 38.99,\n        \"Sensor21\": 23.296\n    }\n}```\n\nTaken together the messages represent a time series of data for multiple engines. Each engine is operating normally at the start of each time series, and develops a fault at some point during the series. The fault grows in magnitude until system failure (i.e. the failure point for the engine is the final cycle in the set). The remaining useful life (RUL) is therefore expressed as: \n\n$$RUL_{current} = Cycle_{max} - Cycle_{current}$$\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create utils for loading data from avro files\n\nDefine a set of utility methods for loading the data from the avro files. We use thes utilities to load the locally downloaded data. Later in the notebook, these same utilities will form the basis of data processing for remote training (see **Train regression using Azure AutoMl and remote execution** below)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile ./utils.py\n\nimport glob\nimport json\nimport pandas as pd\n\nfrom fastavro import reader\nfrom os.path import isfile\nfrom multiprocessing.dummy import Pool as ThreadPool \n\n# parse connectionDeviceId and return the int part\n# (e.g. Client_1 becomes 1)\ndef get_unit_num (unit_record):\n    unit = unit_record[\"connectionDeviceId\"]\n    return int(unit.split('_')[1])\n\n# create data row from avro file record\ndef load_cycle_row(record):\n    json_body = record[\"Body\"].decode()\n    row = json.loads(json_body)\n    row.update({'Unit': get_unit_num(record[\"SystemProperties\"])})\n    row.update({'QueueTime': pd.to_datetime(record[\"EnqueuedTimeUtc\"])})\n    return row\n\n# add row to data frame\ndef append_df(base_df, append_df):\n    if(base_df is None):\n        base_df = pd.DataFrame(append_df)\n    else:\n        base_df = base_df.append(append_df, ignore_index=True)\n    return base_df\n\n# sort rows and columns in dataframe\ndef sort_and_index(index_data):\n    #sort rows and reset index\n    index_data.sort_values(by=['Unit', 'CycleTime'], inplace=True)\n    index_data.reset_index(drop=True, inplace=True)\n    \n    #fix up column sorting for convenience in notebook\n    sorted_cols = ([\"Unit\",\"CycleTime\", \"QueueTime\"] \n                   + [\"OperationalSetting\"+str(i) for i in range(1,4)] \n                   + [\"Sensor\"+str(i) for i in range(1,22)])\n\n    return index_data[sorted_cols]\n\n# load data from an avro file and return a dataframe\ndef load_avro_file(avro_file_name):\n    with open(avro_file_name, 'rb') as fo:\n        file_df = None\n        avro_reader = reader(fo)\n        print (\"load records from file: %s\" % avro_file_name)\n        for record in avro_reader:\n            row = load_cycle_row(record)\n            file_df = append_df(base_df=file_df, append_df=[row])\n        return file_df\n\n# load data from all avro files in given dir \ndef load_avro_directory(avro_dir_name):\n    lst = glob.iglob(avro_dir_name, recursive=True)\n    files = [x for x in lst if isfile(x)]\n    pool = ThreadPool(4)\n    results = pool.map(load_avro_file, files)\n    pool.close()\n    pool.join()\n\n    dir_df = None\n    for df in results:\n        dir_df = append_df(base_df=dir_df, append_df=df)\n    print(\"loaded %d records\" % dir_df.shape[0])\n    return sort_and_index(dir_df)\n\n# add max cycle to each row in the data\ndef add_maxcycle(data_frame):\n    # cleanup column if it already exists\n    if 'MaxCycle' in data_frame.columns:\n        data_frame.drop('MaxCycle', axis=1, inplace=True)\n\n    total_cycles = data_frame.groupby(['Unit']).agg({'CycleTime' : 'max'}).reset_index()\n    total_cycles.rename(columns = {'CycleTime' : 'MaxCycle'}, inplace = True)\n    return data_frame.merge(total_cycles, how = 'left', left_on = 'Unit', right_on = 'Unit')\n\n# return a remaining useful life class based on RUL\ndef classify_rul(rul):\n     if (rul <= 25):\n          return 'F25'\n     elif (rul <= 75):\n          return 'F75'\n     elif (rul <= 150):\n          return 'F150'\n     else:\n          return 'Full'\n    \n# add remaining useful life and remaing useful life class\n# to each row in the data\ndef add_rul(data_frame):\n    data_frame = add_maxcycle(data_frame)\n    \n    if 'RUL' in data_frame.columns:\n        data_frame.drop('RUL', axis=1, inplace=True)\n    data_frame['RUL'] = data_frame.apply(lambda r: int(r['MaxCycle'] - r['CycleTime']), axis = 1)\n\n    if 'RulClass' in data_frame.columns:\n        data_frame.drop('RulClass', axis=1, inplace=True)\n    data_frame['RulClass'] = data_frame.apply(lambda r: classify_rul(r['RUL']), axis = 1)\n    \n    return data_frame\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use utils to load data from download directory\n\nThis step will take several minutes."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import utils\nimport pandas as pd\n\n# check to see if this step has run before and if it has load the data rather than recreating it\nif (os.path.exists('./data/WebServiceTrain.csv')):\n    train_pd = pd.read_csv('./data/WebServiceTrain.csv')\nelse:\n    train_pd =  utils.load_avro_directory('./data/download/**/*')\n\ntrain_pd.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Calculate remaining useful life and RUL class labels\n\nAdd RUL for regression training and RulClass for classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_pd = utils.add_rul(train_pd)\n\ncols = ['Unit', 'CycleTime', 'MaxCycle', 'RUL', 'RulClass']\n#show first 5 rows\ntrain_pd[cols].head(5)\n\n#show last 5 rows for engine 3\ntrain_pd[train_pd['Unit'] == 3][cols].tail(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Display train data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#persist data so we can recover if kernel dies\ntrain_pd.to_csv('./data/WebServiceTrain.csv')\n\n#show the first five rows\ntrain_pd.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Explore the data\n\nVisualize the data to start to get a sense of how features like sensor measurements and operations settings relate to remaining useful life (RUL)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Sensor readings and RUL\n\nCreate a scatterplot for each sensor measurement vs. RUL. Notice that some measurements (e.g. sensor 2) seem to be correlated strongly to RUL whereas other measurements (e.g. sensor 1) stay constant throughout the life of the engine.\n\n    \n><font color=gray>_Note: the data is limited to the first 10 engine units for speed of rendering_</font>"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#select the data to plot\nplotData = train_pd.query('Unit < 10');\n\nsns.set()\ng = sns.PairGrid(data=plotData,\n                x_vars = ['RUL'],\n                y_vars = [\"Sensor\"+str(i) for i in range(1,22)],\n                hue=\"Unit\",\n                height=3,\n                aspect=2.5,\n                palette=\"Paired\")\ng = g.map(plt.scatter, alpha=0.3)\ng = g.set(xlim=(300,0))\ng = g.add_legend()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Operational settings and RUL\n\nCreate a scatterplot for each operation setting vs. RUL. Operational settings do not seem to correlate with RUL.\n    \n><font color=gray>_Note: the data is limited to the first 10 engine units for speed of rendering_</font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport utils\n\nplotData = train_pd.query('Unit < 10');\nsns.set()\ng = sns.PairGrid(data=plotData,\n                x_vars = ['RUL'],\n                y_vars = [\"OperationalSetting\"+str(i) for i in range(1,4)],\n                hue=\"Unit\",\n                height=3,\n                aspect=2.5,\n                palette=\"Paired\")\ng = g.map(plt.scatter, alpha=0.3)\ng = g.set(xlim=(300,0))\ng = g.add_legend()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Train model using Azure AutoMl and remote execution\n\nIn this section, we will use the Azure Machine Learning service to build a model to predict remaining useful life."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##  Create remote compute target\n\nAzure ML Managed Compute is a managed service that enables data scientists to train machine learning models on clusters of Azure virtual machines, including VMs with GPU support. This code creates an Azure Managed Compute cluster if it does not already exist in your workspace. \n\n **Creation of the cluster takes approximately 5 minutes.** If the cluster is already in the workspace this code uses it and skips the creation process."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\nCLUSTER_NAME = \"mlturbo\"\n\n# choose a name for your cluster\nbatchai_cluster_name = CLUSTER_NAME + \"gpu\"\ncluster_min_nodes = 0\ncluster_max_nodes = 3\nvm_size = \"STANDARD_NC6\" #NC6 is GPU-enabled\n      \ncts = ws.compute_targets\nif batchai_cluster_name in cts:\n    found = True\n    print('Found existing compute target...%s' % batchai_cluster_name)\n    compute_target = cts[batchai_cluster_name]\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n                                                               # vm_priority = 'lowpriority', #optional\n                                                                min_nodes = cluster_min_nodes, \n                                                                max_nodes = cluster_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, batchai_cluster_name, provisioning_config)\n    \n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n    # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n    compute_target.status.serialize()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create a regression model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Configure run settings\n\nCreate a DataReferenceConfiguration object to inform the system what data folder to download to the compute target. The path_on_compute should be an absolute path to ensure that the data files are downloaded only once.  The get_data method should use the same path to access the data files."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Setup DataReference"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.automl import AutoMLConfig\nfrom azureml.core.runconfig import DataReferenceConfiguration\n\ndr = DataReferenceConfiguration(datastore_name=ds.name, \n                   path_on_datastore=AZURE_IOT_HUB_NAME, \n                   path_on_compute='/tmp/azureml_runs',\n                   mode='download', # download files from datastore to compute target\n                   overwrite=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Update run settings"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n# create a new RunConfig object\nconda_run_config = RunConfiguration(framework=\"python\")\n\n# Set compute target to the Azure ML managed compute\nconda_run_config.target = compute_target\n\n# set the data reference of the run coonfiguration\nconda_run_config.data_references = {ds.name: dr}\n\n#specify package dependencies needed to load data and train the model\ncd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]', 'fastavro'], conda_packages=['numpy'])\nconda_run_config.environment.python.conda_dependencies = cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create data retrieval script\n\nRemote execution requires a .py file containing a get_data() function that will be used to retrieve data from the mounted storage.  We will create the file for retrieving data by copying the utils.py file to our script folder as get_data.py.  Then we will append a get_data(), which uses the utility methods for data loading, into the newly created get_data.py.  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a directory "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = './turbofan-regression'\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create get data script"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create the script by copyting utils.py to the script_folder\nimport shutil\nshutil.copyfile('utils.py', script_folder + '/get_data.py')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Append the get_data method to the newly created get_data.py\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile -a $script_folder/get_data.py\n\ndef get_data():\n    #for the sake of simplicity use all sensors as training features for the model\n    features = [\"Sensor\"+str(i) for i in range(1,22)]\n    train_pd = load_avro_directory('/tmp/azureml_runs/**/*')\n    train_pd = add_rul(train_pd)\n    y_train = train_pd['RUL'].values\n    X_train = train_pd[features].values\n    return { \"X\" : X_train, \"y\" : y_train}\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Run the experiment on Azure ML compute \n### Instantiate AutoML\n\nIn the interest of time, the cell below uses a short iteration timeout, **1 min**, and a small number of iterations, **10**. Longer iteration timeouts and a greater number of iterations will yield better results"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import logging\nfrom azureml.train.automl import AutoMLConfig\n\n#name project folder and experiment\nexperiment_name = 'turbofan-regression-remote'\n\nautoml_settings = {\n    \"iteration_timeout_minutes\": 2,\n    \"iterations\": 10,\n    \"n_cross_validations\": 10,\n    \"primary_metric\": 'spearman_correlation',\n    \"max_cores_per_iteration\": -1,\n    \"enable_ensembling\": True,\n    \"ensemble_iterations\": 5,\n    \"verbosity\": logging.INFO,\n    \"preprocess\": True,\n    \"enable_tf\": True,\n    \"auto_blacklist\": True\n}\n\nAutoml_config = AutoMLConfig(task = 'regression',\n                             debug_log = 'auto-regress.log',\n                             path=script_folder,\n                             run_configuration=conda_run_config,\n                             data_script = script_folder + \"/get_data.py\",\n                             **automl_settings\n                            )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Run the experiment\n\nRun the experiment on the remote compute target and show results as the runs execute. Assuming you have kept the auto_ml settings set in the notebook this step will take several minutes. If you have increased the number of iterations of the iteration timeout it will take longer.\n\n>Note: unlike other cells, this one is not finished until the \"Status\" in the output below shows \"Completed\". If it shows a failure, you can check the status in the Azure portal (link will be at the bottom of the output) to learn more."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.experiment import Experiment\nfrom azureml.widgets import RunDetails\n\nexperiment=Experiment(ws, experiment_name)\nregression_run = experiment.submit(Automl_config, show_output=True)\nRunDetails(regression_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Explore the results\n\nExplore the results of the automatic training using the run details."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Reconstitute a run\nGiven the long running nature of running the experiment the notebook may have been closed or timed out.  In that case, to retrieve the run from the run id set the value of `run_id` to the run_id of the experiment. We use `AutoMLRun` from `azureml.train.automl.run`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.automl.run import AutoMLRun\nfrom azureml.core.experiment import Experiment\nfrom azureml.core import Workspace\n\nrun_id = 'AutoML_xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n\nif 'regression_run' not in globals():\n    ws = Workspace.from_config()\n    experiment_name = 'turbofan-regression-remote'\n    experiment=Experiment(ws, experiment_name)\n    regression_run = AutoMLRun(experiment = experiment, \n                               run_id = run_id)\n\nregression_run.id",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Retrieve all iterations\n\nView the experiment history and see individual metrics for each iteration run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "children = list(regression_run.get_children())\nmetricslist = {}\nfor run in children:\n    properties = run.get_properties()\n    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n    metricslist[int(properties['iteration'])] = metrics\n\nimport pandas as pd\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Register the best model \n\nUse the `regression_run` object to get the best model and register it into the workspace. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = regression_run.get_output()\n\n# register model in workspace\ndescription = 'Aml Model ' + regression_run.id[7:15]\ntags = None\nregression_run.register_model(description=description, tags=tags)\nregression_run.model_id # Use this id to deploy the model as a web service in Azure",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Save model information for deployment\n\nPersist the information that we will need to deploy the model in the [turbofan deploy model](./turbofan_deploy_model.ipynb)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nimport os\n\nmodel_information = {'regressionRunId': regression_run.id, 'modelId': regression_run.model_id, 'experimentName': experiment.name}\nwith open('./aml_config/model_config.json', 'w') as fo:\n  json.dump(model_information, fo)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Load test data\n\nIn the test set, the time series ends some time prior to system failure. The actual\nremaining useful life (RUL) are given in the RUL_*.txt files.  The data in the RUL files is a single vector where the index corresponds to the unit number of the engine and the value corresponds to the actual RUL at the end of the test.\n\nThe RUL for a given cycle in the training set is given by adding the RUL at test end (from the RUL vector file) to the maximum cycle in the test data and then subtracting the current cycle:\n\n$$RUL_{current} =  RUL_{TestEnd} + Cycle_{max} - Cycle_{current}$$\n\nTaking unit number 1 as an example:\n   * Taking the first value from RUL_FD003.txt gives:       $RUL_{TestEnd} = 44$\n   * The final(max) cycle value from test_FD003.txt gives:  $Cycle_{max} = 233$\n   * The values for the first 5 cycles for engine 1 are:\n\n|Unit|Cycle|Max Cycle|Test End RUL|Remaining Life|\n|-----|-----|-----|-----|-----|\n|1|1|233|44|276|\n|1|2|233|44|275|\n|1|3|233|44|274|\n|1|4|233|44|273|\n|1|5|233|44|272|\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n## Define some methods for loading from text files"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nimport utils\nimport pandas as pd\nfrom os.path import isfile\n\ndef add_column_names(data_frame):\n    data_frame.columns = ([\"Unit\",\"CycleTime\"]\n                          + [\"OperationalSetting\"+str(i) for i in range(1,4)]\n                          + [\"Sensor\"+str(i) for i in range(1,22)])\n\ndef read_data_file(full_file_name):\n    data = pd.read_csv(full_file_name, sep = ' ', header = None)\n    data.dropna(axis='columns', inplace=True)\n    return data\n\ndef load_rul_data(full_file_name):\n    rul_data = read_data_file(full_file_name)\n\n    # add a column for the unit and fill with numbers 1..n where\n    # n = number of rows of RUL data\n    rul_data['Unit'] = list(range(1, len(rul_data) + 1))\n    rul_data.rename(columns = {0 : 'TestEndRUL'}, inplace = True)\n    return rul_data\n\n\ndef load_test_data(test_full_file_name, rul_full_file_name):\n    data = read_data_file(test_full_file_name)\n    add_column_names(data)\n    data = utils.add_maxcycle(data)\n    \n    rul_data = load_rul_data(rul_full_file_name)\n    data = data.merge(rul_data, how = 'left', left_on = 'Unit', right_on = 'Unit')\n    data['RUL'] = data.apply(lambda r: int(r['MaxCycle'] + r['TestEndRUL'] - r['CycleTime']), axis = 1)\n    data['RulClass'] = data.apply(lambda r: utils.classify_rul(r['RUL']), axis = 1)\n\n    return data\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Read and process the test data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset = \"FD003\" \nrul_file_name = 'data/RUL_' + dataset + '.txt'\ntest_file_name = 'data/test_' + dataset + '.txt'\n\ntest_pd = load_test_data(test_file_name, rul_file_name)\ntest_pd.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Serialize test data\n\nSave off the data so that we can use it when we test the web service in the [turbofan deploy model](./turbofan_deploy_model.ipynb) notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_pd.to_csv('./data/WebServiceTest.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Test regression model\n\npredict on training and test set and calculate residual values"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "selected_features = [\"Sensor\"+str(i) for i in range(1,22)]\n\n#reload data in case the kernel died at some point\nif 'train_pd' not in globals():\n    train_pd = pd.read_csv(\"data/WebServiceTrain.csv\")\n\n#load the values used to train the model\nX_train = train_pd[selected_features].values\ny_train = train_pd['RUL'].values\n\n#predict and calculate residual values for train\ny_pred_train = fitted_model.predict(X_train)\ny_residual_train = y_train - y_pred_train\n\ntrain_pd['predicted'] = y_pred_train; \ntrain_pd['residual'] = y_residual_train\n\n#load the values from the test set\nX_test = test_pd[selected_features].values\ny_test = test_pd['RUL'].values\n\n#predict and calculate residual values for test\ny_pred_test = fitted_model.predict(X_test)\ny_residual_test = y_test - y_pred_test\n\ntest_pd['predicted'] = y_pred_test;\ntest_pd['residual'] = y_residual_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_pd[['Unit', 'RUL', 'predicted', 'residual']].head(5)\ntest_pd[['Unit', 'RUL', 'predicted', 'residual']].head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Predicted vs. actual\n\nPlot the predicted RUL against the actual RUL.  The dashed line represents the ideal model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfig, (ax1,ax2) = plt.subplots(nrows=2, sharex=True)\nfig.set_size_inches(16, 16)\n\nfont_size = 14\n\ng = sns.regplot(y='predicted', x='RUL', data=train_pd, fit_reg=False, ax=ax1)\nlim_set = g.set(ylim=(0, 500), xlim=(0, 500))\nplot = g.axes.plot([0, 500], [0, 500], c=\".3\", ls=\"--\");\n\nrmse = ax1.text(16,450,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_pred_train))), fontsize = font_size)\nr2 = ax1.text(16,425,'R2 Score = {0:.2f}'.format(r2_score(y_train, y_pred_train)), fontsize = font_size)\n\ng2 = sns.regplot(y='predicted', x='RUL', data=test_pd, fit_reg=False, ax=ax2)\nlim_set = g2.set(ylim=(0, 500), xlim=(0, 500))\nplot = g2.axes.plot([0, 500], [0, 500], c=\".3\", ls=\"--\");\n\nrmse = ax2.text(16,450,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_pred_test))), fontsize = font_size)\nr2 = ax2.text(16,425,'R2 Score = {0:.2f}'.format(r2_score(y_test, y_pred_test)), fontsize = font_size)\n\nptitle = ax1.set_title('Train data', size=font_size)\nxlabel = ax1.set_xlabel('Actual RUL', size=font_size)\nylabel = ax1.set_ylabel('Predicted RUL', size=font_size)\n\nptitle = ax2.set_title(\"Test data\", size=font_size)\nxlabel = ax2.set_xlabel('Actual RUL', size=font_size)\nylabel = ax2.set_ylabel('Predicted RUL', size=font_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Predicted vs. residual"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig, (ax1,ax2) = plt.subplots(nrows=2, sharex=True)\nfig.set_size_inches(16, 16)\n\nfont_size = 14\n\ng = sns.regplot(y='residual', x='predicted', data=train_pd, fit_reg=False, ax=ax1)\nlim_set = g.set(ylim=(-350, 350), xlim=(0, 350))\nplot = g.axes.plot([0, 350], [0, 0], c=\".3\", ls=\"--\");\n\ng2 = sns.regplot(y='residual', x='predicted', data=test_pd, fit_reg=False, ax=ax2)\nlim_set = g2.set(ylim=(-350, 350), xlim=(0, 350))\nplot = g2.axes.plot([0, 350], [0, 0], c=\".3\", ls=\"--\");\n\nptitle = ax1.set_title('Train data', size=font_size)\nxlabel = ax1.set_xlabel('Predicted RUL', size=font_size)\nylabel = ax1.set_ylabel('Residual', size=font_size)\n\nptitle = ax2.set_title(\"Test data\", size=font_size)\nxlabel = ax2.set_xlabel('Predicted RUL', size=font_size)\nylabel = ax2.set_ylabel('Residual', size=font_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Residual distribution\n\nPlot histogram and Q-Q plot for test and train data to check for normal distibution of residuals"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import statsmodels.api as sm\nimport scipy.stats as stats\n\nfig, (ax1,ax2) = plt.subplots(nrows=2, ncols= 2)\nfig.set_size_inches(16, 16)\n\ng = sns.distplot(train_pd['residual'], ax=ax1[0], kde=False)\ng = stats.probplot(train_pd['residual'], plot=ax1[1])\n\ng2 = sns.distplot(test_pd['residual'], ax=ax2[0], kde=False)\ng2 = stats.probplot(test_pd['residual'], plot=ax2[1])\n\n\nptitle = ax1[0].set_title('Residual Histogram Train', size=font_size)\nxlabel = ax1[0].set_xlabel('Residuals', size=font_size)\nptitle = ax1[1].set_title('Q-Q Plot Train Residuals', size=font_size)\n\nptitle = ax2[0].set_title('Residual Histogram Test', size=font_size)\nxlabel = ax2[0].set_xlabel('Residuals', size=font_size)\nptitle = ax2[1].set_title('Q-Q Plot Test Residuals', size=font_size)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Next Steps\n\nNow that we have a working model we want to deploy it as an Azure IoT Edge module.  The [turbofan deploy model](./02-turbofan_deploy_model.ipynb) walks through the steps to create and Edge module."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}